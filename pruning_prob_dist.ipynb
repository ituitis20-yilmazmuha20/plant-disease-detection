{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes:  64\n",
      "trained data dir:  ./DATASETS/merged_resized_pngs_splited_augmented/train\n",
      "number of classes:  64\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"./\"\n",
    "TRAINED_DATA_DIR = os.path.join(BASE_PATH, 'DATASETS/merged_resized_pngs_splited_augmented/train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_PATH, 'DATASETS/merged_resized_pngs_splited_augmented/test')\n",
    "NUM_CLASSES = len(os.listdir(os.path.join(BASE_PATH, f\"{TRAINED_DATA_DIR}\")))\n",
    "\n",
    "print(\"number of classes: \", NUM_CLASSES)\n",
    "print(\"trained data dir: \", TRAINED_DATA_DIR)\n",
    "\n",
    "\n",
    "def load_model_from_local(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = 'models/MobileNetV2_20240531_021014/best_model.pth'\n",
    "mobilenetv2_base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenetv2_base_model.classifier[1] = nn.Linear(mobilenetv2_base_model.last_channel, NUM_CLASSES)\n",
    "mobilenetv2_model = load_model_from_local(mobilenetv2_base_model, MODEL_PATH)\n",
    "mobilenetv2_model.eval()\n",
    "mobilenetv2_model.to(device)\n",
    "\n",
    "classes = os.listdir(os.path.join(BASE_PATH, TRAINED_DATA_DIR))\n",
    "classes.sort()\n",
    "print(\"number of classes: \", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example labels:  [4, 5]\n",
      "before prune:  tensor([[ 4.1964e-02, -8.4229e-03,  7.2694e-01,  1.0631e+00, -1.0895e+00,\n",
      "          1.0599e+01, -6.0295e-01,  6.2821e-01,  5.2467e-01,  2.8852e-01,\n",
      "         -1.1933e+00,  5.8391e-01, -1.7345e+00,  1.0210e-01,  5.2058e-01,\n",
      "         -1.5617e+00,  8.0694e-02,  9.7825e-01,  2.9813e-01,  6.3134e-01,\n",
      "         -3.9957e-01, -8.3195e-01, -9.0069e-01,  1.2850e+00, -5.6000e-01,\n",
      "         -5.2574e-01, -7.5184e-01,  1.6021e-01, -3.8565e-01, -1.4111e+00,\n",
      "         -1.2495e+00, -1.3900e+00, -9.1956e-01, -5.5036e-01,  1.5790e+00,\n",
      "         -1.2418e+00, -7.0023e-01, -5.2419e-01, -1.6087e-01, -6.1729e-01,\n",
      "         -6.0234e-01, -6.7896e-01, -1.0629e+00, -1.3607e+00, -3.8894e-01,\n",
      "          2.6982e-02,  6.6836e-01, -8.4422e-01, -2.2255e-01,  1.6911e+00,\n",
      "          1.9547e-01, -1.6722e-02,  4.2677e-04,  3.6578e+00,  6.6889e-01,\n",
      "          1.2079e-01, -3.2807e-01, -4.1721e-01, -3.4990e-02,  5.0905e-01,\n",
      "          8.2462e-02, -8.4921e-01, -6.4979e-01, -7.8063e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "after prune:  tensor([[-1.0895, 10.5990]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "predicted class index:  5\n",
      "predicted class:  Cherry__powdery_mildew\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(TRAINED_DATA_DIR, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = ImageFolder(TEST_DATA_DIR, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "plant_names = [class_name.split(\"__\")[0] for class_name in class_names]\n",
    "\n",
    "def find_labels_by_plant_name(plant_name, class_names):\n",
    "    labels = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if class_name.split(\"__\")[0] == plant_name:\n",
    "            labels.append(i)\n",
    "    return labels \n",
    "\n",
    "example_image_path_1 = os.path.join(BASE_PATH, '0c8432e0-0484-470c-a774-7cce596b9e64___JR_FrgE.S 2841.png')\n",
    "example_image_path_2 = os.path.join(BASE_PATH, '0a5aacba-0363-4b71-9beb-30183982d415___FREC_Pwd.M 4919_1.png')\n",
    "example_plant_name_1 = \"Apple\"\n",
    "example_plant_name_2 = \"Cherry\"\n",
    "\n",
    "example_labels = find_labels_by_plant_name(example_plant_name_2, class_names)\n",
    "print(\"example labels: \", example_labels)\n",
    "\n",
    "mobilenetv2_model.eval()\n",
    "image = Image.open(example_image_path_2)\n",
    "image = transform(image).unsqueeze(0)\n",
    "image = image.to(device)\n",
    "output = mobilenetv2_model(image)\n",
    "\n",
    "# prune output\n",
    "print(\"before prune: \", output)\n",
    "output = output[:, example_labels]\n",
    "print(\"after prune: \", output)\n",
    "_, predicted_pruned_index  = torch.max(output, 1)\n",
    "predicted_class_index = example_labels[predicted_pruned_index.item()]\n",
    "print(\"predicted class index: \", predicted_class_index)\n",
    "predicted_class = class_names[predicted_class_index]\n",
    "print(\"predicted class: \", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.75%\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "mobilenetv2_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenetv2_model(images)\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            example_labels  = find_labels_by_plant_name(plant_names[labels[i].item()], class_names)\n",
    "            output = outputs[i, example_labels]\n",
    "            _, predicted = torch.max(output, 0)\n",
    "            predicted_class_index = example_labels[predicted.item()]\n",
    "\n",
    "            if predicted_class_index == labels[i].item():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            \n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
