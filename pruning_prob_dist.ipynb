{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes:  64\n",
      "trained data dir:  ./DATASETS/merged_resized_pngs_splited_augmented/train\n",
      "number of classes:  64\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"./\"\n",
    "TRAINED_DATA_DIR = os.path.join(BASE_PATH, 'DATASETS/merged_resized_pngs_splited_augmented/train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_PATH, 'DATASETS/merged_resized_pngs_splited_augmented/test')\n",
    "NUM_CLASSES = len(os.listdir(os.path.join(BASE_PATH, f\"{TRAINED_DATA_DIR}\")))\n",
    "\n",
    "print(\"number of classes: \", NUM_CLASSES)\n",
    "print(\"trained data dir: \", TRAINED_DATA_DIR)\n",
    "\n",
    "\n",
    "def load_model_from_local(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = 'models/(trained on augmented)MobileNetV2_20240531_021014/best_model.pth'\n",
    "mobilenetv2_base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenetv2_base_model.classifier[1] = nn.Linear(mobilenetv2_base_model.last_channel, NUM_CLASSES)\n",
    "mobilenetv2_model = load_model_from_local(mobilenetv2_base_model, MODEL_PATH)\n",
    "mobilenetv2_model.eval()\n",
    "mobilenetv2_model.to(device)\n",
    "\n",
    "classes = os.listdir(os.path.join(BASE_PATH, TRAINED_DATA_DIR))\n",
    "classes.sort()\n",
    "print(\"number of classes: \", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_image_path_1 = os.path.join(BASE_PATH, \\'0c8432e0-0484-470c-a774-7cce596b9e64___JR_FrgE.S 2841.png\\')\\nexample_image_path_2 = os.path.join(BASE_PATH, \\'0a5aacba-0363-4b71-9beb-30183982d415___FREC_Pwd.M 4919_1.png\\')\\nexample_plant_name_1 = \"Apple\"\\nexample_plant_name_2 = \"Cherry\"\\n\\nexample_labels = find_labels_by_plant_name(example_plant_name_2, class_names)\\nprint(\"example labels: \", example_labels)\\n\\nmobilenetv2_model.eval()\\nimage = Image.open(example_image_path_2)\\nimage = transform(image).unsqueeze(0)\\nimage = image.to(device)\\noutput = mobilenetv2_model(image)\\n\\n# prune output\\nprint(\"before prune: \", output)\\noutput = output[:, example_labels]\\nprint(\"after prune: \", output)\\n_, predicted_pruned_index  = torch.max(output, 1)\\npredicted_class_index = example_labels[predicted_pruned_index.item()]\\nprint(\"predicted class index: \", predicted_class_index)\\npredicted_class = class_names[predicted_class_index]\\nprint(\"predicted class: \", predicted_class)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ImageFolder(TRAINED_DATA_DIR, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = ImageFolder(TEST_DATA_DIR, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "plant_names = [class_name.split(\"__\")[0] for class_name in class_names]\n",
    "\n",
    "def find_labels_by_plant_name(plant_name, class_names):\n",
    "    labels = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if class_name.split(\"__\")[0] == plant_name:\n",
    "            labels.append(i)\n",
    "    return labels \n",
    "\n",
    "\"\"\"example_image_path_1 = os.path.join(BASE_PATH, '0c8432e0-0484-470c-a774-7cce596b9e64___JR_FrgE.S 2841.png')\n",
    "example_image_path_2 = os.path.join(BASE_PATH, '0a5aacba-0363-4b71-9beb-30183982d415___FREC_Pwd.M 4919_1.png')\n",
    "example_plant_name_1 = \"Apple\"\n",
    "example_plant_name_2 = \"Cherry\"\n",
    "\n",
    "example_labels = find_labels_by_plant_name(example_plant_name_2, class_names)\n",
    "print(\"example labels: \", example_labels)\n",
    "\n",
    "mobilenetv2_model.eval()\n",
    "image = Image.open(example_image_path_2)\n",
    "image = transform(image).unsqueeze(0)\n",
    "image = image.to(device)\n",
    "output = mobilenetv2_model(image)\n",
    "\n",
    "# prune output\n",
    "print(\"before prune: \", output)\n",
    "output = output[:, example_labels]\n",
    "print(\"after prune: \", output)\n",
    "_, predicted_pruned_index  = torch.max(output, 1)\n",
    "predicted_class_index = example_labels[predicted_pruned_index.item()]\n",
    "print(\"predicted class index: \", predicted_class_index)\n",
    "predicted_class = class_names[predicted_class_index]\n",
    "print(\"predicted class: \", predicted_class)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# testing \n",
    "mobilenetv2_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenetv2_model(images)\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            example_labels  = find_labels_by_plant_name(plant_names[labels[i].item()], class_names)\n",
    "            output = outputs[i, example_labels]\n",
    "            _, predicted = torch.max(output, 0)\n",
    "            predicted_class_index = example_labels[predicted.item()]\n",
    "\n",
    "            true_labels.append(labels[i].item())\n",
    "            predicted_labels.append(predicted_class_index)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.7454%\n",
      "Precision: 96.7799%\n",
      "Recall: 96.7454%\n",
      "F1 Score: 96.7405%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = sum(1 for x, y in zip(true_labels, predicted_labels) if x == y) / len(true_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.4f}%')\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "print(f'Precision: {precision * 100:.4f}%')\n",
    "print(f'Recall: {recall * 100:.4f}%')\n",
    "print(f'F1 Score: {f1 * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 96.7454%\n",
    "Precision: 96.7799%\n",
    "Recall: 96.7454%\n",
    "F1 Score: 96.7405%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
